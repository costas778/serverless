https://aws-tc-largeobjects.s3.us-west-2.amazonaws.com/DEV-AWS-MO-Architecting/exercise-1-serverless.html

Task 1. Setup: Creating IAM policies and roles
When you first create an account on AWS, you become a root user, or an account owner. We don’t recommend that you use the account root user for daily operations and tasks. Instead, you should use an IAM user or IAM roles to access specific services and features. IAM policies, users, and roles are offered at no additional charge.

In this task, you create custom IAM policies and roles to grant limited permissions to specific AWS services.

Step 1.1: Creating custom IAM policies
Sign in to the AWS Management Console.

In the search box, enter IAM.

From the results list, choose IAM.

In the navigation pane, choose Policies.

Choose Create policy.

The Create policy page appears. You can create and edit a policy in the visual editor or use JSON. In this exercise, we provide JSON scripts to create policies. In total, you must create four policies.

In the JSON tab, paste the following code:

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "dynamodb:PutItem",
                "dynamodb:DescribeTable"
            ],
            "Resource": "*"
        }
    ]
}
Copy to clipboard
This JSON script grants permissions to put items into the DynamoDB table. The asterisk (*) indicates that the specified actions can apply to all available resources.

Choose Next: Tags and then choose Next: Review.

For the policy name, enter Lambda-Write-DynamoDB.

Choose Create policy.

After you create the Lambda-Write-DynamoDB policy, repeat the previous steps to create the following policies:

A policy for Amazon SNS to get, list, and publish topics that are received by Lambda:

Name: Lambda-SNS-Publish
JSON:
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "sns:Publish",
                "sns:GetTopicAttributes",
                    "sns:ListTopics"
            ],
                "Resource": "*"
        }
    ]
 }
Copy to clipboard
A policy for Lambda to get records from DynamoDB Streams:

Name: Lambda-DynamoDBStreams-Read
JSON:
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "dynamodb:GetShardIterator",
                "dynamodb:DescribeStream",
                "dynamodb:ListStreams",
                "dynamodb:GetRecords"
            ],
            "Resource": "*"
        }
    ]
}
Copy to clipboard
A policy for Lambda to read messages that are placed in Amazon SQS:

Name: Lambda-Read-SQS
JSON:
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "sqs:DeleteMessage",
                "sqs:ReceiveMessage",
                "sqs:GetQueueAttributes",
                "sqs:ChangeMessageVisibility"
            ],
            "Resource": "*"
        }
    ]
}
Copy to clipboard
Step 1.2: Creating IAM roles and attaching policies to the roles
Because AWS follows the principle of least privilege, we recommend that you provide role-based access to only the AWS resources that are required to perform a task. In this step, you create IAM roles and attach policies to the roles.

In the navigation pane of the IAM dashboard, choose Roles.

Choose Create role and in the Select trusted entity page, configure the following settings:
Trusted entity type: AWS service
Common use cases: Lambda
Choose Next.

On the Add permissions page, select Lambda-Write-DynamoDB and Lambda-Read-SQS.

Choose Next

For Role name, enter Lambda-SQS-DynamoDB.

Choose Create role.

Follow the previous steps to create two more IAM roles:

An IAM role for AWS Lambda: This role grants permissions to obtain records from the DynamoDB streams and send the records to Amazon SNS. Use the following information to create the role.
IAM role name: Lambda-DynamoDBStreams-SNS
Trusted entity type: AWS service
Common use cases: Lambda
Attach policies: Lambda-SNS-Publish and Lambda-DynamoDBStreams-Read
An IAM role for Amazon API Gateway: This role grants permissions to send data to the SQS queue and push logs to Amazon CloudWatch for troubleshooting. Use the following information to create the role.
IAM role name: APIGateway-SQS
Trusted entity type: AWS service
Common use cases: API Gateway
Attach policies: AmazonAPIGatewayPushToCloudWatchLogs
Task 2: Creating a DynamoDB table
In this task, you create a DynamoDB table that ingests data that’s passed on through API Gateway.

In the search box of the AWS Management Console, enter DynamoDB.

From the list, choose the DynamoDB service.

On the Get started card, choose Create table and configure the following settings:
Table: orders
Partition key: orderID
Data type: Keep String
Keep the remaining settings at their default values, and choose Create table.

Task 3: Creating an SQS queue
In this task, you create an SQS queue. In the architecture for this exercise, the Amazon SQS receives data records from API Gateway, stores them, and then sends them to a database.

In the AWS Management Console search box, enter SQS and from the list, choose Simple Queue Service.

On the Get started card, choose Create queue.

The Create queue page appears.

Configure the following settings:
Name: POC-Queue
Access Policy: Basic
Define who can send messages to the queue:
Select Only the specified AWS accounts, IAM users and roles
In the box for this option, paste the Amazon Resource Name (ARN) for the APIGateway-SQS IAM role
Note: For example, your IAM role might look similar to the following: arn:aws:iam::<account ID>:role/APIGateway-SQS.
Define who can receive messages from the queue:
Select Only the specified AWS accounts, IAM users and roles.
In the box for this option, paste the ARN for the Lambda-SQS-DynamoDB IAM role.
Note: For example, your IAM role might look similar to the following: arn:aws:iam::<account_ID>:role/Lambda-SQS-DynamoDB
Choose Create queue.

Task 4: Creating a Lambda function and setting up triggers
In this task, you create a Lambda function that reads messages from the SQS queue and writes an order record to the DynamoDB table.

Step 4.1: Creating a Lambda function for the Lambda-SQS-DynamoDB role
In the AWS Management Console search box, enter Lambda and from the list, choose Lambda.

Choose Create function and configure the following settings:
Function option: Author from scratch
Function name: POC-Lambda-1
Runtime: Python 3.9
Change default execution role: Use an existing role
Existing role: Lambda-SQS-DynamoDB
Choose Create function.

Step 4.2: Setting up Amazon SQS as a trigger to invoke the function
If needed, expand the Function overview section.

Choose Add trigger.

For Trigger configuration, enter SQS and choose the service in the list.

For SQS queue, choose POC-Queue.

Add the trigger by choosing Add.

Step 4.3: Adding and deploying the function code
On the POC-Lambda-1 page, in the Code tab, replace the default Lambda function code with the following code:

import boto3, uuid

client = boto3.resource('dynamodb')
table = client.Table("orders")

def lambda_handler(event, context):
    for record in event['Records']:
        print("test")
        payload = record["body"]
        print(str(payload))
        table.put_item(Item= {'orderID': str(uuid.uuid4()),'order':  payload})
Copy to clipboard
Choose Deploy.

The Lambda code passes arguments to a function call. As a result, when a trigger invokes a function, Lambda runs the code that you specify.

When you use Lambda, you are responsible only for your code. Lambda manages the memory, CPU, network, and other resources to run your code.

Step 4.4: Testing the POC-Lambda-1 Lambda function
In the Test tab, create a new event that has the following settings:
Event name: POC-Lambda-Test-1
Template-Optional: SQS
The SQS template appears in the Event JSON field.

Save your changes and choose Test.

After the Lambda function runs successfully, the “Execution result: succeeded” message appears in the notification banner in the Test section. This means that the Lambda function sent a test message “Hello from SQS!” from the SQS template to the DynamoDB table.

Step 4.5: Verifying that the Lambda function adds the test message to a database
In the AWS Management Console search box, enter DynamoDB and from the list, choose DynamoDB.

In the navigation pane, choose Explore items.

Select the orders database. Under Items returned, the orders table returns “Hello from SQS!” from the Lambda function test.

Task 5: Enabling DynamoDB Streams
In this task, you enable DynamoDB Streams. A DynamoDB stream captures information about every modification to data items in the table.

In the DynamoDB console, in the Tables section of the navigation pane, choose Update settings.

In the Tables card, make sure that the orders table is selected.

Choose the Exports and streams tab.

In the DynamoDB stream details section, choose Enable.

For View type, choose New image.

Choose Enable stream.

After the Lambda function reads messages from the SQS queue and writes an order record to the DynamoDB table, DynamoDB Streams captures the primary key attributes from the record.

Task 6: Creating an SNS topic and setting up subscriptions
In this task, you create an SNS topic and set up subscriptions. Amazon SNS coordinates and manages delivering or sending messages to subscriber endpoints or clients.

Step 6.1: Creating a topic in the notification service
In the AWS Management Console, search for SNS and choose Simple Notification Service.

On the Create topic card, enter POC-Topic and choose Next step.

In the Details section, keep the Standard topic type selected and choose Create topic.

On the POC-Topic page, copy the ARN of the topic that you just created and save it for your reference.

You will need the ARN for the SNS topic later in this exercise.

Step 6.2: Subscribing to email notifications
On the Subscriptions tab, choose Create subscription.

For Topic ARN, make sure that the box contains the ARN for POC-Topic.

To receive notifications, for Protocol, choose Email.

For Endpoint, enter your email address.

Choose Create subscription.

The confirmation message is sent to the email address that you specified.

After you receive the confirmation email message, confirm the subscription. If you don’t receive an email message within a few minutes, check the spam folder.

Task 7: Creating an AWS Lambda function to publish a message to the SNS topic
In this task, you create a Lambda function for the Lambda-DynamoDBStreams-SNS role. The second Lambda function uses DynamoDB Streams as a trigger to pass the record of a new entry to Amazon SNS.

Step 7.1: Creating a POC-Lambda-2 function
In the AWS Management Console, search for and open AWS Lambda.

Create a new Lambda function by choosing Create function, and configure the following settings:
Function option: Author from scratch
Function name: POC-Lambda-2
Runtime: Python 3.9
Change default execution role: Use an existing role
Existing role: Lambda-DynamoDBStreams-SNS
This role grants permissions to get records from DynamoDB Streams and send them to Amazon SNS.

Choose Create function.

Step 7.2: Setting up DynamoDB as a trigger to invoke a Lambda function
In the Function overview section, choose Add trigger and configure the following settings:
Trigger configuration: Enter DynamoDB and from the list, choose DynamoDB.
DynamoDB table: orders
Keep the remaining default settings and choose Add.

In the Configuration tab, make sure that you are in the Triggers section and that the DynamoDB state is “Enabled.”

Step 7.3: Configuring the second Lambda function
Choose the Code tab and replace the Lambda function code with the following code:

import boto3, json

client = boto3.client('sns')

def lambda_handler(event, context):

    for record in event["Records"]:

        if record['eventName'] == 'INSERT':
            new_record = record['dynamodb']['NewImage']    
            response = client.publish(
                TargetArn='<Enter Amazon SNS ARN for the POC-Topic>',
                Message=json.dumps({'default': json.dumps(new_record)}),
                MessageStructure='json'
            )
Copy to clipboard
Note: In the function code, replace the TargetArn value with the ARN for the Amazon SNS POC-Topic. Make sure that you remove the placeholder angle brackets (<>).
Note
The exercises in this course will have an associated charge in your AWS account. In this exercise, you will create the following resources:

AWS Identity and Access Management (IAM) policy and user (policies and users are AWS account features, offered at no additional charge)
Amazon DynamoDB table
AWS Lambda functions
Amazon Simple Queue Service (Amazon SQS) queue
Amazon Simple Notification Service (Amazon SNS) topic
Amazon API Gateway
Amazon CloudWatch Logs

The final task in this exercise includes instructions to delete all the resources that you create.

Familiarize yourself with DynamoDB pricing, Amazon SQS pricing, Amazon SNS pricing, Lambda pricing, API Gateway pricing, CloudWatch Logs pricing, and the AWS Free Tier.

Exercise 1. Architecting Solutions: Building a Proof of Concept for a Serverless Solution
This exercise provides you with instructions for how to build a proof of concept for a serverless solution in the AWS Cloud.

Suppose you have a customer that needs a serverless web backend hosted on AWS. The customer sells cleaning supplies and often sees spikes in demand for their website, which means that they need an architecture that can easily scale in and out as demand changes. The customer also wants to ensure that the application has decoupled application components.

The following architectural diagram shows the flow for the serverless solution that you will build.

Architecture diagram for exercise 1
Architecture diagram for exercise 1
In this architecture, you will use a REST API to place a database entry in the Amazon SQS queue. Amazon SQS will then invoke the first Lambda function, which inserts the entry into a DynamoDB table. After that, DynamoDB Streams will capture a record of the new entry in a database and invoke a second Lambda function. The function will pass the database entry to Amazon SNS. After Amazon SNS processes the new record, it will send you a notification through a specified email address.

In this exercise, you will learn how to do the following:

Create IAM policies and roles to follow best practices of working in the AWS Cloud.
Create a DynamoDB table to store data.
Create an Amazon SQS queue to receive, store, and send messages between software components.
Create Lambda functions and set up triggers to invoke actions in different AWS services.
Enable DynamoDB Streams to capture modifications in the database table.
Configure Amazon SNS to receive email or text notifications.
Create a REST API to insert data into a database.
Notes:

To complete the instructions in this exercise, choose the US East (N. Virginia) us-east-1 Region in the navigation pane of the AWS Management Console.

The instructions might prompt you to enter your account ID. Your account ID is a 12-digit account number that appears under your account alias in the top-right corner of the AWS Management Console. When you enter your account number (ID), make sure that you remove hyphens (-).

Task 1. Setup: Creating IAM policies and roles
When you first create an account on AWS, you become a root user, or an account owner. We don’t recommend that you use the account root user for daily operations and tasks. Instead, you should use an IAM user or IAM roles to access specific services and features. IAM policies, users, and roles are offered at no additional charge.

In this task, you create custom IAM policies and roles to grant limited permissions to specific AWS services.Note
The exercises in this course will have an associated charge in your AWS account. In this exercise, you will create the following resources:

AWS Identity and Access Management (IAM) policy and user (policies and users are AWS account features, offered at no additional charge)
Amazon DynamoDB table
AWS Lambda functions
Amazon Simple Queue Service (Amazon SQS) queue
Amazon Simple Notification Service (Amazon SNS) topic
Amazon API Gateway
Amazon CloudWatch Logs

The final task in this exercise includes instructions to delete all the resources that you create.

Familiarize yourself with DynamoDB pricing, Amazon SQS pricing, Amazon SNS pricing, Lambda pricing, API Gateway pricing, CloudWatch Logs pricing, and the AWS Free Tier.

Exercise 1. Architecting Solutions: Building a Proof of Concept for a Serverless Solution
This exercise provides you with instructions for how to build a proof of concept for a serverless solution in the AWS Cloud.

Suppose you have a customer that needs a serverless web backend hosted on AWS. The customer sells cleaning supplies and often sees spikes in demand for their website, which means that they need an architecture that can easily scale in and out as demand changes. The customer also wants to ensure that the application has decoupled application components.

The following architectural diagram shows the flow for the serverless solution that you will build.

Architecture diagram for exercise 1
Architecture diagram for exercise 1
In this architecture, you will use a REST API to place a database entry in the Amazon SQS queue. Amazon SQS will then invoke the first Lambda function, which inserts the entry into a DynamoDB table. After that, DynamoDB Streams will capture a record of the new entry in a database and invoke a second Lambda function. The function will pass the database entry to Amazon SNS. After Amazon SNS processes the new record, it will send you a notification through a specified email address.

In this exercise, you will learn how to do the following:

Create IAM policies and roles to follow best practices of working in the AWS Cloud.
Create a DynamoDB table to store data.
Create an Amazon SQS queue to receive, store, and send messages between software components.
Create Lambda functions and set up triggers to invoke actions in different AWS services.
Enable DynamoDB Streams to capture modifications in the database table.
Configure Amazon SNS to receive email or text notifications.
Create a REST API to insert data into a database.
Notes:

To complete the instructions in this exercise, choose the US East (N. Virginia) us-east-1 Region in the navigation pane of the AWS Management Console.

The instructions might prompt you to enter your account ID. Your account ID is a 12-digit account number that appears under your account alias in the top-right corner of the AWS Management Console. When you enter your account number (ID), make sure that you remove hyphens (-).

Task 1. Setup: Creating IAM policies and roles
When you first create an account on AWS, you become a root user, or an account owner. We don’t recommend that you use the account root user for daily operations and tasks. Instead, you should use an IAM user or IAM roles to access specific services and features. IAM policies, users, and roles are offered at no additional charge.

In this task, you create custom IAM policies and roles to grant limited permissions to specific AWS services.

Step 1.1: Creating custom IAM policies
Sign in to the AWS Management Console.

In the search box, enter IAM.

From the results list, choose IAM.

In the navigation pane, choose Policies.

Choose Create policy.

The Create policy page appears. You can create and edit a policy in the visual editor or use JSON. In this exercise, we provide JSON scripts to create policies. In total, you must create four policies.

In the JSON tab, paste the following code:

{
    "Version": "2012-10-17",
From the results list, choose IAM.

In the navigation pane, choose Policies.

Choose Create policy.

The Create policy page appears. You can create and edit a policy in the visual editor or use JSON. In this exercise, we provide JSON scripts to create policies. In total, you must create four policies.

In the JSON tab, paste the following code:

{
    "Version": "2012-10-17",ns:us-east-1:<account ID>:POC-Topic.

Choose Deploy.

Step 7.4: Testing the POC-Lambda-2 Lambda function
On the Test tab, create a new event and for Event name, enter POC-Lambda-Test-2.

For Template-optional, enter DynamoDB and from the list, choose DynamoDB-Update.

The DynamoDB template appears in the Event JSON box.

Save your changes and choose Test.

After the Lambda function successfully runs, the “Execution result: succeeded” message should appear in the notification banner in the Test section.

In a few minutes, an email message should arrive at the email address that you specified in the previous task.

Confirm that you received the subscription email message. If needed, check both your inbox and spam folder.

Task 8: Creating an API with Amazon API Gateway
In this task, you create a REST API in Amazon API Gateway. The API serves as a communication gateway between your application and the AWS services.

In the AWS Management Console, search for and open API Gateway.

On the REST API card with a public authentication, choose Build and configure the following settings:
Choose the protocol: REST
Create new API: New API
API name: POC-API
Endpoint Type: Regional
Choose Create API.

On the Actions menu, choose Create Method.

Open the method menu by choosing the down arrow, and choose POST. Save your changes by choosing the check mark.

In the POST - Setup pane, configure the following settings:
Integration type: AWS Service
AWS Region: us-east-1
AWS Service: Simple Queue Service (SQS)
AWS Subdomain: Keep empty
HTTP method: POST
Action Type: Use path override
Path override: Enter your account ID followed by a slash (/) and the name of the POC queue
Note: If POC-Queue is the name of the SQS queue that you created, this entry might look similar to the following: /<account ID>/POC-Queue
Execution role: Paste the ARN of the APIGateway-SQS role
Note: For example, the ARN might look like the following: arn:aws:iam::<account ID>:role/APIGateway-SQS
Content Handling: Passthrough
Save your changes.

Choose the Integration Request card.

Scroll to the bottom of the page and expand HTTP Headers.

Choose Add header.

For Name, enter Content-Type

For Mapped from, enter 'application/x-www-form-urlencoded'

Save your changes to the HTTP Headers section by choosing the check mark.

Expand Mapping Templates and for Request body passthrough, choose Never.

Choose Add mapping template and for Content-Type , enter application/json

Save your changes by choosing the check mark.

For Generate template, do not choose a default template from the list. Instead, enter the following command: Action=SendMessage&MessageBody=$input.body in a box.

Choose Save.

Task 9: Testing the architecture by using API Gateway
In this task, you use API Gateway to send mock data to Amazon SQS as a proof of concept for the serverless solution.

In the API Gateway console, return to the POST - Method Execution page and choose Test.

In the Request Body box, enter:

{  "item": "latex gloves",
"customerID":"12345"}
Copy to clipboard
Choose Test.

If you see the “Successfully completed execution” message with the 200 response in the logs on the right, you will receive an email notification with the new entry. If you don’t receive an email, but the new item appears in the DynamoDB table, troubleshoot the exercise instructions starting from after you set up DynamoDB. Ensure that you deploy all of the resources in the us-east-1 Region.

After API Gateway successfully processes the request that you pasted in the Request Body box, it places the request in the SQS queue. Because you set up Amazon SQS as a trigger in the first Lambda function, Amazon SQS invokes the function call. The Lambda function code places the new entry into the DynamoDB table. DynamoDB Streams captures this change to the database and invokes the second AWS Lambda function. This function gets the new record from DynamoDB Streams and sends it to Amazon SNS. Amazon SNS, in turn, sends you an email notification.

//////////////////////////Start here///////////////////////////////////////////////////

And update the root main.tf module calls with the necessary variables.

To use this infrastructure:

Create the Lambda function code and zip files in the appropriate locations

POC-Lambda-1

import boto3, uuid

client = boto3.resource('dynamodb')
table = client.Table("orders")

def lambda_handler(event, context):
    for record in event['Records']:
        print("test")
        payload = record["body"]
        print(str(payload))
        table.put_item(Item= {'orderID': str(uuid.uuid4()),'order':  payload})


There was a second in the sinstructions

import boto3, json

client = boto3.client('sns')

def lambda_handler(event, context):

    for record in event["Records"]:

        if record['eventName'] == 'INSERT':
            new_record = record['dynamodb']['NewImage']    
            response = client.publish(
                TargetArn='<Enter Amazon SNS ARN for the POC-Topic>',
                Message=json.dumps({'default': json.dumps(new_record)}),
                MessageStructure='json'
            )



Copy to clipboard




Initialize Terraform:

terraform init


Insert at cursor

Create a terraform.tfvars file with your values:

aws_region = "us-east-1"
account_id = "your-account-id"
notification_email = "your-email@example.com"
environment = "poc"


Insert at cursor

Plan and apply:

terraform plan
terraform apply



Insert at cursor

Remember to confirm your SNS subscription via email after deployment


/////////////////////

Now in Task 3 when this task is done manually to create the SQS role you need to obtain the arn strings. How will this be automated?

Configure the following settings:
Name: POC-Queue
Access Policy: Basic
Define who can send messages to the queue:
Select Only the specified AWS accounts, IAM users and roles
In the box for this option, paste the Amazon Resource Name (ARN) for the APIGateway-SQS IAM role
Note: For example, your IAM role might look similar to the following: arn:aws:iam::<account ID>:role/APIGateway-SQS.
Define who can receive messages from the queue:
Select Only the specified AWS accounts, IAM users and roles.
In the box for this option, paste the ARN for the Lambda-SQS-DynamoDB IAM role.
Note: For example, your IAM role might look similar to the following: arn:aws:iam::<account_ID>:role/Lambda-SQS-DynamoDB

//////////////////////////
In the Terraform configuration, we can automate this by modifying 
the SQS module to use the role ARNs that are created by the IAM module. 
Here's how to modify the SQS module:

modules/sqs/main.tf:

resource "aws_sqs_queue" "poc_queue" {
  name                      = "${var.environment}-queue"
  delay_seconds             = 0
  max_message_size         = 262144
  message_retention_seconds = 345600 # 4 days
  receive_wait_time_seconds = 0
  visibility_timeout_seconds = 30

  tags = var.tags
}

resource "aws_sqs_queue_policy" "poc_queue_policy" {
  queue_url = aws_sqs_queue.poc_queue.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Sid = "APIGatewayAccess"
        Effect = "Allow"
        Principal = {
          AWS = var.api_gateway_role_arn
        }
        Action = "sqs:SendMessage"
        Resource = aws_sqs_queue.poc_queue.arn
      },
      {
        Sid = "LambdaAccess"
        Effect = "Allow"
        Principal = {
          AWS = var.lambda_sqs_role_arn
        }
        Action = [
          "sqs:ReceiveMessage",
          "sqs:DeleteMessage",
          "sqs:GetQueueAttributes"
        ]
        Resource = aws_sqs_queue.poc_queue.arn
      }
    ]
  })
}

Update modules/sqs/variables.tf:

variable "environment" {
  description = "Environment name"
  type        = string
}

variable "api_gateway_role_arn" {
  description = "ARN of the API Gateway role"
  type        = string
}

variable "lambda_sqs_role_arn" {
  description = "ARN of the Lambda SQS role"
  type        = string
}

variable "tags" {
  description = "Tags to apply to resources"
  type        = map(string)
  default     = {}
}

Then update the root main.tf to pass the role ARNs:

module "sqs" {
  source = "./modules/sqs"
  
  environment         = var.environment
  api_gateway_role_arn = module.iam.api_gateway_role_arn
  lambda_sqs_role_arn  = module.iam.lambda_sqs_role_arn
  tags                = var.tags

  depends_on = [module.iam]
}

This configuration:

Uses the role ARNs created by the IAM module

Automatically sets up the SQS queue access policy

Ensures proper permissions for both API Gateway to send messages 
and Lambda to receive messages

Uses depends_on to ensure the IAM roles exist before creating the SQS 
queue policy

The ARNs are automatically passed between modules, eliminating the
 need for manual copying and pasting of ARN strings.



////////////////////////

Looking at the text file, in Steps 6.1 and 6.2, the ARNs need to be 
obtained for:

The DynamoDB table ARN

The SNS topic ARN

In Terraform, this is handled automatically through module outputs 
and references. Here's how:

For the DynamoDB table ARN:

# In modules/lambda/main.tf, we can reference the DynamoDB ARN through
 environment variables
resource "aws_lambda_function" "sqs_to_dynamodb" {
  # ... other configuration ...

  environment {
    variables = {
      DYNAMODB_TABLE_ARN = var.dynamodb_table_arn
      DYNAMODB_TABLE_NAME = var.dynamodb_table_name
    }
  }
}

# The ARN is passed from the DynamoDB module output
# In modules/dynamodb/outputs.tf
output "table_arn" {
  value = aws_dynamodb_table.poc_table.arn
}

# In root main.tf, we pass it to the Lambda module
module "lambda" {
  source = "./modules/lambda"
  dynamodb_table_arn = module.dynamodb.table_arn
  # ... other variables ...
}

This way, instead of manually copying and pasting ARNs as shown 
in Steps 6.1 and 6.2 of the manual process, Terraform:

Automatically generates the ARNs when creating the resources

Makes them available through module outputs

Passes them to the Lambda functions through environment variables

Maintains the correct dependencies between resources

The Lambda function code can then access these ARNs through 
environment variables:

// For the first Lambda function
const dynamodbTableArn = process.env.DYNAMODB_TABLE_ARN;
const dynamodbTableName = process.env.DYNAMODB_TABLE_NAME;

// For the second Lambda function
const snsTopicArn = process.env.SNS_TOPIC_ARN;

This eliminates the need for manual ARN handling and reduces the 
possibility of errors from manual copying.


Yes, we need to update a few files to properly handle the ARNs for 
the Lambda functions. Here are the necessary changes:

Update modules/lambda/main.tf:

# First Lambda function - SQS to DynamoDB
resource "aws_lambda_function" "sqs_to_dynamodb" {
  filename         = "${path.module}/functions/sqs_to_dynamodb.zip"
  function_name    = "${var.environment}-sqs-to-dynamodb"
  role            = var.lambda_sqs_role_arn
  handler         = "index.handler"
  runtime         = "nodejs14.x"
  timeout         = 30

  environment {
    variables = {
      DYNAMODB_TABLE_ARN = var.dynamodb_table_arn
      DYNAMODB_TABLE_NAME = var.dynamodb_table_name
      AWS_REGION = var.aws_region
    }
  }

  tags = var.tags
}

# Second Lambda function - DynamoDB Streams to SNS
resource "aws_lambda_function" "dynamodb_to_sns" {
  filename         = "${path.module}/functions/dynamodb_to_sns.zip"
  function_name    = "${var.environment}-dynamodb-to-sns"
  role            = var.lambda_dynamodb_role_arn
  handler         = "index.handler"
  runtime         = "nodejs14.x"
  timeout         = 30

  environment {
    variables = {
      SNS_TOPIC_ARN = var.sns_topic_arn
      AWS_REGION = var.aws_region
    }
  }

  tags = var.tags
}

Copy

Insert at cursor
hcl


Update modules/lambda/variables.tf:

# Add these variables if not already present
variable "dynamodb_table_name" {
  description = "Name of the DynamoDB table"
  type        = string
}

variable "dynamodb_table_arn" {
  description = "ARN of the DynamoDB table"
  type        = string
}

variable "aws_region" {
  description = "AWS region"
  type        = string
}

Copy

Insert at cursor
hcl
Update root main.tf Lambda module call:

module "lambda" {
  source = "./modules/lambda"
  
  environment              = var.environment
  aws_region              = var.aws_region
  lambda_sqs_role_arn     = module.iam.lambda_sqs_role_arn
  lambda_dynamodb_role_arn = module.iam.lambda_dynamodb_role_arn
  dynamodb_table_arn      = module.dynamodb.table_arn
  dynamodb_table_name     = module.dynamodb.table_name
  dynamodb_stream_arn     = module.dynamodb.stream_arn
  sqs_queue_arn          = module.sqs.queue_arn
  sns_topic_arn          = module.sns.topic_arn
  tags                    = var.tags

  depends_on = [
    module.dynamodb,
    module.sqs,
    module.sns,
    module.iam
  ]
}

module "lambda" {
  source = "./modules/lambda"
  dynamodb_table_arn = module.dynamodb.table_arn
  sqs_queue_arn = module.sqs.queue_arn
  sns_topic_arn = module.sns.topic_arn
  lambda_sqs_role_arn = module.iam.lambda_sqs_role_arn
  lambda_dynamodb_role_arn = module.iam.lambda_dynamodb_role_arn
  depends_on = [module.dynamodb, module.sqs, module.sns, module.iam]
}

Copy

Insert at cursor
hcl
Ensure modules/dynamodb/outputs.tf includes:

output "table_arn" {
  description = "ARN of the DynamoDB table"
  value       = aws_dynamodb_table.poc_table.arn
}

output "table_name" {
  description = "Name of the DynamoDB table"
  value       = aws_dynamodb_table.poc_table.name
}

output "stream_arn" {
  description = "ARN of the DynamoDB stream"
  value       = aws_dynamodb_table.poc_table.stream_arn
}

Copy

Insert at cursor
hcl
These changes ensure that:

The Lambda functions have access to all necessary ARNs via environment variables

The ARNs are properly passed between modules

The region is available to the Lambda functions

All dependencies are properly managed

The Lambda function code can then access these values through environment variables without needing manual ARN configuration.



////////////////////////

The current Terraform configuration already handles this through 
environment variables, but let's make sure the Lambda function code 
matches the environment variable approach.

Here's how it should be configured:

First, ensure the Lambda module is receiving the SNS Topic ARN 
(which is already done in our current configuration): [1]

# in root main.tf
module "lambda" {
  # ... other configuration ...
  sns_topic_arn = module.sns.topic_arn
  # ... other configuration ...
}

Copy

Insert at cursor
hcl
The Lambda function code should be updated to use the environment 
variable instead of hardcoding the ARN. Create this file as 
modules/lambda/functions/dynamodb_to_sns/index.py:

import boto3
import json
import os

client = boto3.client('sns')
SNS_TOPIC_ARN = os.environ['SNS_TOPIC_ARN']

def lambda_handler(event, context):
    for record in event["Records"]:
        if record['eventName'] == 'INSERT':
            new_record = record['dynamodb']['NewImage']    
            response = client.publish(
                TargetArn=SNS_TOPIC_ARN,
                Message=json.dumps({'default': json.dumps(new_record)}),
                MessageStructure='json'
            )
    return {
        'statusCode': 200,
        'body': json.dumps('Message published to SNS successfully!')
    }

Copy

Insert at cursor
python
Make sure the Lambda function has the environment variable set 
(which is already in our configuration):

# in modules/lambda/main.tf
resource "aws_lambda_function" "dynamodb_to_sns" {
  filename         = "${path.module}/functions/dynamodb_to_sns.zip"
  function_name    = "${var.environment}-dynamodb-to-sns"
  role            = var.lambda_dynamodb_role_arn
  handler         = "index.lambda_handler"
  runtime         = "python3.9"
  timeout         = 30

  environment {
    variables = {
      SNS_TOPIC_ARN = var.sns_topic_arn
    }
  }

  tags = var.tags
}

Copy

Insert at cursor
hcl
The current Terraform configuration already handles the ARN passing
 correctly. The main change needed is in the actual Lambda function
  code to use the environment variable instead of requiring manual 
  ARN insertion.

You'll need to:

Create the Python file as shown above

Zip it before deploying:

cd modules/lambda/functions/dynamodb_to_sns
zip -r ../dynamodb_to_sns.zip index.py

Copy

Insert at cursor
bash
This approach:

Eliminates manual ARN copying

Makes the function more portable

Reduces the chance of errors

Makes it easier to deploy to different environments

Follows AWS best practices for configuration management

The Terraform configuration itself doesn't need updating as it 
already handles the ARN passing correctly through environment variables.

Yes, the Terraform configuration already handles this ARN passing, 
but let's verify and make sure it's properly configured in the API 
Gateway module. Here's the relevant part that needs to be checked:

First, verify the API Gateway module configuration in 
modules/api_gateway/main.tf:

resource "aws_api_gateway_integration" "sqs_integration" {
  rest_api_id             = aws_api_gateway_rest_api.poc_api.id
  resource_id             = aws_api_gateway_resource.poc_resource.id
  http_method             = aws_api_gateway_method.poc_method.http_method
  integration_http_method = "POST"
  type                   = "AWS"
  
  # This handles the path override requirement
  uri                    = "arn:aws:apigateway:${var.aws_region}:sqs:path/${var.account_id}/${var.sqs_queue_name}"
  
  # This handles the execution role ARN requirement
  credentials            = var.api_gateway_role_arn

  request_parameters = {
    "integration.request.header.Content-Type" = "'application/x-www-form-urlencoded'"
  }

  request_templates = {
    "application/json" = "Action=SendMessage&MessageBody=$input.body"
  }
}

Copy

Insert at cursor
hcl
Make sure the variables are properly defined in 
modules/api_gateway/variables.tf:

variable "aws_region" {
  description = "AWS region"
  type        = string
}

variable "account_id" {
  description = "AWS account ID"
  type        = string
}

variable "api_gateway_role_arn" {
  description = "ARN of the API Gateway role"
  type        = string
}

variable "sqs_queue_name" {
  description = "Name of the SQS queue"
  type        = string
}

Copy

Insert at cursor
hcl
Update the root main.tf to pass all required values:

module "api_gateway" {
  source = "./modules/api_gateway"
  
  environment          = var.environment
  aws_region          = var.aws_region
  account_id          = var.account_id
  api_gateway_role_arn = module.iam.api_gateway_role_arn
  sqs_queue_name      = module.sqs.queue_name
  tags                = var.tags

  depends_on = [
    module.iam,
    module.sqs
  ]
}

Copy

Insert at cursor
hcl
Ensure the SQS module outputs the queue name in modules/sqs/outputs.tf:

output "queue_name" {
  description = "Name of the SQS queue"
  value       = aws_sqs_queue.poc_queue.name
}

Copy

Insert at cursor
hcl
This configuration:

Automatically constructs the correct SQS URI path using the account ID and queue name

Uses the API Gateway role ARN created by the IAM module [1]

Eliminates the need for manual ARN copying

Maintains proper dependencies between resources

The values are automatically passed through the module system, so you don't need to manually copy and paste any ARNs or account IDs. Everything is handled through variables and module outputs.

###########################################

The Lambda function code should be saved in your project structure under the Lambda module directory. Here's the correct location and structure: [1]

project_root/
├── modules/
│   ├── lambda/
│   │   ├── functions/
│   │   │   ├── sqs_to_dynamodb/
│   │   │   │   └── index.js        # First Lambda (POC-Lambda-1)
│   │   │   └── dynamodb_to_sns/
│   │   │       └── index.js        # Second Lambda
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── outputs.tf
│   └── [other modules...]
├── main.tf
├── variables.tf
├── outputs.tf
└── terraform.tfvars

Copy

Insert at cursor
text
First Lambda (POC-Lambda-1) should be saved as: modules/lambda/functions/sqs_to_dynamodb/index.js:

import boto3, uuid

client = boto3.resource('dynamodb')
table = client.Table("orders")

def lambda_handler(event, context):
    for record in event['Records']:
        print("test")
        payload = record["body"]
        print(str(payload))
        table.put_item(Item= {'orderID': str(uuid.uuid4()),'order':  payload})

Copy

Insert at cursor
javascript
Second Lambda should be saved as: modules/lambda/functions/dynamodb_to_sns/index.js:

import boto3, json
import os

client = boto3.client('sns')
SNS_TOPIC_ARN = os.environ['SNS_TOPIC_ARN']

def lambda_handler(event, context):
    for record in event["Records"]:
        if record['eventName'] == 'INSERT':
            new_record = record['dynamodb']['NewImage']    
            response = client.publish(
                TargetArn=SNS_TOPIC_ARN,
                Message=json.dumps({'default': json.dumps(new_record)}),
                MessageStructure='json'
            )

Copy

Insert at cursor
javascript
After saving these files, you need to create the zip files:

# From your project root
cd modules/lambda/functions/sqs_to_dynamodb
zip -r ../sqs_to_dynamodb.zip index.js

cd ../dynamodb_to_sns
zip -r ../dynamodb_to_sns.zip index.js

Copy

Insert at cursor
bash
These zip files will then be used by the Terraform Lambda module configuration we created earlier. 
The filename parameter in the Lambda resource configurations points to these zip files: [2]

resource "aws_lambda_function" "sqs_to_dynamodb" {
  filename         = "${path.module}/functions/sqs_to_dynamodb.zip"
  # ... other configuration
}

resource "aws_lambda_function" "dynamodb_to_sns" {
  filename         = "${path.module}/functions/dynamodb_to_sns.zip"
  # ... other configuration
}

Copy

Insert at cursor
hcl
Remember to:

Create the directory structure

Save both Lambda functions in their respective directories

Create the zip files before running Terraform

Make sure the file names and paths match what's specified in your Terraform configuratio

//////////////////////////////////

To verify these configurations in API Gateway, you can use several methods: [1]

Using Terraform Plan:

# Run terraform plan to see if variables are resolving correctly
terraform plan

# Or for specific module
terraform plan -target=module.api_gat

Using AWS CLI to verify after deployment:

# Get the REST API ID first
aws apigateway get-rest-apis

# Then use the API ID to get integration details
aws apigateway get-integration \
    --rest-api-id <your-api-id> \
    --resource-id <your-resource-id> \
    --http-method POST

# Check the role attached
aws apigateway get-rest-api \
    --rest-api-id <your-api-id>

Using Terraform State:

# Show the current state of the API Gateway integration
terraform show -json | jq '.values.root_module.module_calls.api_gateway'

# Or using state list and show
terraform state list | grep api_gateway
terraform state show 'module.api_gateway.aws_api_gateway_integration.sqs_integration'

Manual verification in AWS Console:

Go to API Gateway console

Select your API

Click on Resources

Select the integration

Verify these specific points:

Integration type should be "AWS"

URI path should match your expected format

IAM role should match your expected role ARN

Testing the variables in Terraform:

# Add this to your api_gateway/main.tf temporarily for testing
output "debug_integration_uri" {
  value = "arn:aws:apigateway:${var.aws_region}:sqs:path/${var.account_id}/${var.sqs_queue_name}"

Verify variable definitions:

# Check if variables are defined in terraform.tfvars or similar
cat terraform.tfvars

# Expected content similar to:
aws_region = "us-east-1"
account_id = "123456789012"
sqs_queue_name = "my-queue"
api_gateway_role_arn = "arn:aws:iam::123456789012:role/api-gateway-role"

Test the API endpoint:

# Get the invoke URL
aws apigateway get-stages --rest-api-id <your-api-id>

# Test with curl
curl -X POST \
  'https://<api-id>.execute-api.<region>.amazonaws.com/<stage>/<resource>' \
  -H 'Content-Type: application/json' \
  -d '{"test": "message"}'

If any of these verifications fail:

Check variable values in terraform.tfvars

Verify IAM role permissions

Ensure SQS queue exists and is accessible

Check API Gateway logs for any integration errors

Verify region and account ID are correct

Remember to remove any temporary debug outputs after verification is complete.


